{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial framework taken from https://github.com/jaara/AI-blog/blob/master/CartPole-A3C.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = 'LunarLander-v2'\n",
    "CONTINUOUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_CLIPPING = 0.2 # Only implemented clipping for the surrogate loss, paper said it was best\n",
    "EPOCHS = 10\n",
    "NOISE = 1.0 # Exploration noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 2048\n",
    "BATCH_SIZE = 256\n",
    "NUM_ACTIONS = 4\n",
    "NUM_STATE = 8\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 2\n",
    "ENTROPY_LOSS = 5e-3\n",
    "LR = 1e-4  # Lower lr stabilises training greatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY_ACTION, DUMMY_VALUE = np.zeros((1, NUM_ACTIONS)), np.zeros((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit\n",
    "def exponential_average(old, new, b1):\n",
    "    return old * b1 + (1-b1) * new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximal_policy_optimization_loss(advantage, old_prediction):\n",
    "    def loss(y_true, y_pred):\n",
    "        prob = K.sum(y_true * y_pred, axis=-1)\n",
    "        old_prob = K.sum(y_true * old_prediction, axis=-1)\n",
    "        r = prob/(old_prob + 1e-10)\n",
    "        return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage) + ENTROPY_LOSS * -(prob * K.log(prob + 1e-10)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximal_policy_optimization_loss_continuous(advantage, old_prediction):\n",
    "    def loss(y_true, y_pred):\n",
    "        var = K.square(NOISE)\n",
    "        pi = 3.1415926\n",
    "        denom = K.sqrt(2 * pi * var)\n",
    "        prob_num = K.exp(- K.square(y_true - y_pred) / (2 * var))\n",
    "        old_prob_num = K.exp(- K.square(y_true - old_prediction) / (2 * var))\n",
    "\n",
    "        prob = prob_num/denom\n",
    "        old_prob = old_prob_num/denom\n",
    "        r = prob/(old_prob + 1e-10)\n",
    "\n",
    "        return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.critic = self.build_critic()\n",
    "        if CONTINUOUS is False:\n",
    "            self.actor = self.build_actor()\n",
    "        else:\n",
    "            self.actor = self.build_actor_continuous()\n",
    "\n",
    "        self.env = gym.make(ENV)\n",
    "        print(self.env.action_space, 'action_space', self.env.observation_space, 'observation_space')\n",
    "        self.episode = 0\n",
    "        self.observation = self.env.reset()\n",
    "        self.val = False\n",
    "        self.reward = []\n",
    "        self.reward_over_time = []\n",
    "        self.name = self.get_name()\n",
    "        self.writer = SummaryWriter(self.name)\n",
    "        self.gradient_steps = 0\n",
    "\n",
    "    def get_name(self):\n",
    "        name = 'AllRuns/'\n",
    "        if CONTINUOUS is True:\n",
    "            name += 'continous/'\n",
    "        else:\n",
    "            name += 'discrete/'\n",
    "        name += ENV\n",
    "        return name\n",
    "\n",
    "    def build_actor(self):\n",
    "        state_input = Input(shape=(NUM_STATE,))\n",
    "        advantage = Input(shape=(1,))\n",
    "        old_prediction = Input(shape=(NUM_ACTIONS,))\n",
    "\n",
    "        x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)\n",
    "        for _ in range(NUM_LAYERS - 1):\n",
    "            x = Dense(HIDDEN_SIZE, activation='tanh')(x)\n",
    "\n",
    "        out_actions = Dense(NUM_ACTIONS, activation='softmax', name='output')(x)\n",
    "\n",
    "        model = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n",
    "        model.compile(optimizer=Adam(lr=LR),\n",
    "                      loss=[proximal_policy_optimization_loss(\n",
    "                          advantage=advantage,\n",
    "                          old_prediction=old_prediction)])\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_actor_continuous(self):\n",
    "        state_input = Input(shape=(NUM_STATE,))\n",
    "        advantage = Input(shape=(1,))\n",
    "        old_prediction = Input(shape=(NUM_ACTIONS,))\n",
    "\n",
    "        x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)\n",
    "        for _ in range(NUM_LAYERS - 1):\n",
    "            x = Dense(HIDDEN_SIZE, activation='tanh')(x)\n",
    "\n",
    "        out_actions = Dense(NUM_ACTIONS, name='output', activation='tanh')(x)\n",
    "\n",
    "        model = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n",
    "        model.compile(optimizer=Adam(lr=LR),\n",
    "                      loss=[proximal_policy_optimization_loss_continuous(\n",
    "                          advantage=advantage,\n",
    "                          old_prediction=old_prediction)])\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        state_input = Input(shape=(NUM_STATE,))\n",
    "        x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)\n",
    "        for _ in range(NUM_LAYERS - 1):\n",
    "            x = Dense(HIDDEN_SIZE, activation='tanh')(x)\n",
    "\n",
    "        out_value = Dense(1)(x)\n",
    "\n",
    "        model = Model(inputs=[state_input], outputs=[out_value])\n",
    "        model.compile(optimizer=Adam(lr=LR), loss='mse')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def reset_env(self):\n",
    "        self.episode += 1\n",
    "        if self.episode % 100 == 0:\n",
    "            self.val = True\n",
    "        else:\n",
    "            self.val = False\n",
    "        self.observation = self.env.reset()\n",
    "        self.reward = []\n",
    "\n",
    "    def get_action(self):\n",
    "        p = self.actor.predict([self.observation.reshape(1, NUM_STATE), DUMMY_VALUE, DUMMY_ACTION])\n",
    "        if self.val is False:\n",
    "\n",
    "            action = np.random.choice(NUM_ACTIONS, p=np.nan_to_num(p[0]))\n",
    "        else:\n",
    "            action = np.argmax(p[0])\n",
    "        action_matrix = np.zeros(NUM_ACTIONS)\n",
    "        action_matrix[action] = 1\n",
    "        return action, action_matrix, p\n",
    "\n",
    "    def get_action_continuous(self):\n",
    "        p = self.actor.predict([self.observation.reshape(1, NUM_STATE), DUMMY_VALUE, DUMMY_ACTION])\n",
    "        if self.val is False:\n",
    "            action = action_matrix = p[0] + np.random.normal(loc=0, scale=NOISE, size=p[0].shape)\n",
    "        else:\n",
    "            action = action_matrix = p[0]\n",
    "        return action, action_matrix, p\n",
    "\n",
    "    def transform_reward(self):\n",
    "        if self.val is True:\n",
    "            self.writer.add_scalar('Val episode reward', np.array(self.reward).sum(), self.episode)\n",
    "        else:\n",
    "            self.writer.add_scalar('Episode reward', np.array(self.reward).sum(), self.episode)\n",
    "        for j in range(len(self.reward) - 2, -1, -1):\n",
    "            self.reward[j] += self.reward[j + 1] * GAMMA\n",
    "\n",
    "    def get_batch(self):\n",
    "        batch = [[], [], [], []]\n",
    "\n",
    "        tmp_batch = [[], [], []]\n",
    "        while len(batch[0]) < BUFFER_SIZE:\n",
    "            if CONTINUOUS is False:\n",
    "                action, action_matrix, predicted_action = self.get_action()\n",
    "            else:\n",
    "                action, action_matrix, predicted_action = self.get_action_continuous()\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            self.reward.append(reward)\n",
    "\n",
    "            tmp_batch[0].append(self.observation)\n",
    "            tmp_batch[1].append(action_matrix)\n",
    "            tmp_batch[2].append(predicted_action)\n",
    "            self.observation = observation\n",
    "\n",
    "            if done:\n",
    "                self.transform_reward()\n",
    "                if self.val is False:\n",
    "                    for i in range(len(tmp_batch[0])):\n",
    "                        obs, action, pred = tmp_batch[0][i], tmp_batch[1][i], tmp_batch[2][i]\n",
    "                        r = self.reward[i]\n",
    "                        batch[0].append(obs)\n",
    "                        batch[1].append(action)\n",
    "                        batch[2].append(pred)\n",
    "                        batch[3].append(r)\n",
    "                tmp_batch = [[], [], []]\n",
    "                self.reset_env()\n",
    "\n",
    "        obs, action, pred, reward = np.array(batch[0]), np.array(batch[1]), np.array(batch[2]), np.reshape(np.array(batch[3]), (len(batch[3]), 1))\n",
    "        pred = np.reshape(pred, (pred.shape[0], pred.shape[2]))\n",
    "        return obs, action, pred, reward\n",
    "\n",
    "    def run(self):\n",
    "        while self.episode < EPISODES:\n",
    "            obs, action, pred, reward = self.get_batch()\n",
    "            obs, action, pred, reward = obs[:BUFFER_SIZE], action[:BUFFER_SIZE], pred[:BUFFER_SIZE], reward[:BUFFER_SIZE]\n",
    "            old_prediction = pred\n",
    "            pred_values = self.critic.predict(obs)\n",
    "\n",
    "            advantage = reward - pred_values\n",
    "\n",
    "            actor_loss = self.actor.fit([obs, advantage, old_prediction], [action], batch_size=BATCH_SIZE, shuffle=True, epochs=EPOCHS, verbose=False)\n",
    "            critic_loss = self.critic.fit([obs], [reward], batch_size=BATCH_SIZE, shuffle=True, epochs=EPOCHS, verbose=False)\n",
    "            self.writer.add_scalar('Actor loss', actor_loss.history['loss'][-1], self.gradient_steps)\n",
    "            self.writer.add_scalar('Critic loss', critic_loss.history['loss'][-1], self.gradient_steps)\n",
    "\n",
    "            self.gradient_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ag = Agent()\n",
    "    ag.run()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
